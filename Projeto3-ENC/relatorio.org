#+LATEX_HEADER: \usepackage{mathtools} 
#+LATEX_HEADER: \DeclarePairedDelimiter\brackets{(}{)}
#+LATEX_HEADER: \DeclarePairedDelimiter\curly{\{}{\}}
#+LATEX_COMPILER: xelatex
#+TITLE: Projeto 3 ENC
\break
* exercicio 1.1

*Let $X \sim$ Weibull $(\alpha, \beta)$, with $\beta$ known and $\alpha$ unknown, which has p.d.f.*

\[
f(x ; \alpha, \beta)=\frac{\beta}{\alpha}\brackets*{\frac{x}{\alpha}}^{\beta-1} e^{-\brackets*{\frac{x}{\alpha}}^\beta}, \quad \alpha, \beta>0, \quad x \in \mathbb{R}^{+} .
\]

*(a) Show that this distribution belongs to the exponential family.*

We want to show that the weibull distribution with $\alpha$ unknown and $\beta$ known belongs to the the exponential family. For that to happen we must show that:

\[
f(x;\alpha,\beta) = exp\curly*{ \frac{\eta(\alpha)T(x) - A(\alpha)}{b(\beta)}+ c(x,\beta)}
\]

then:
\begin{align*}
&f\brackets*{x;\alpha,\beta} = \frac{\beta}{\alpha} \brackets*{\frac{x}{\alpha}}^{\beta - 1} e^{-\brackets*{ \frac{x}{\alpha}}^\beta} =\\
&= \exp \curly*{\log \brackets*{ \frac{\beta}{\alpha} \brackets*{\frac{x}{\alpha}}^{\beta - 1} e^{-\brackets*{ \frac{x}{\alpha}}^\beta}}} \\
&= \exp \curly*{ \log \brackets*{ \frac{\beta}{\alpha} \brackets*{\frac{x}{\alpha}}^{\beta - 1} } + \log \brackets*{ e^{-\brackets*{ \frac{x}{\alpha}}^\beta}}}\\
&= \exp \curly*{ \log \brackets*{ \frac{\beta}{\alpha} \brackets*{\frac{x}{\alpha}}^{\beta - 1} } -\brackets*{ \frac{x}{\alpha}}^\beta } \\
&= \exp \curly*{ \brackets*{\log \brackets*{ \frac{\beta}{\alpha}  } + \log \brackets*{\frac{x}{\alpha}}^{\beta - 1}} -\brackets*{ \frac{x}{\alpha}}^\beta } \\
&= \exp \curly*{ \brackets*{\log \brackets*{ \frac{\beta}{\alpha}  } + \brackets*{ \beta - 1 }\log \brackets*{\frac{x}{\alpha}}} -\brackets*{ \frac{x}{\alpha}}^\beta } \\
&= \exp \curly*{ \brackets*{\log \brackets*{ \frac{\beta}{\alpha}  } + \brackets*{ \beta - 1 } \brackets*{\log\brackets*{x}-\log\brackets*{\alpha}}} -\brackets*{ \frac{x}{\alpha}}^\beta } \\
&= \exp \curly*{ \brackets*{\log \brackets*{ \frac{\beta}{\alpha}  } + \brackets*{ \beta - 1 } \brackets*{\log\brackets*{x}-\log\brackets*{\alpha}}} -\brackets*{ \frac{x}{\alpha}}^\beta } \\
&= \exp \curly*{\log \brackets*{ \frac{\beta}{\alpha}  } + \brackets*{ \beta - 1 } \log\brackets*{x}-\brackets*{ \beta - 1 }\log\brackets*{\alpha} -\brackets*{ \frac{x}{\alpha}}^\beta } \\
&= \exp \curly*{\log \brackets*{ \frac{\beta}{\alpha}  } + \brackets*{ \beta - 1 } \log\brackets*{x}-\brackets*{ \beta - 1 }\log\brackets*{\alpha} - x^\beta \alpha^{-\beta} }\\
&= \exp \curly*{-\alpha^{-\beta} x^\beta -\brackets*{\brackets*{ \beta - 1 }\log\brackets*{\alpha} -\log \brackets*{ \frac{\beta}{\alpha}  }} + \brackets*{ \beta - 1 } \log\brackets*{x}}
\end{align*}
where:
\begin{align*}
&\eta\brackets*{\alpha} = -\alpha^{-\beta}\\
&T\brackets*{x} = x^\beta\\
&A\brackets*{\alpha} = \brackets*{\beta - 1}\log\brackets*{\alpha} - \log\brackets*{\frac{\beta}{\alpha}}\\
&c(x;\beta) = \brackets*{\beta - 1} \log\brackets*{x}\\
&b(\beta) = 1
\end{align*}

\break
*(b) Clearly identify the canonical link and the sufficient statistic. Do you already have the canonical form? If not, write it down.*

$\eta = -\alpha^{-\beta}$ is the canonical link and $T(x) = x^\beta$ is the sufficient statistic. Because $\eta(\alpha) \neq \alpha$, we learn that the previous result isn't yet in the canonical form. Applying the cannonical transformation $\eta(\alpha) = -\alpha^{-\beta}$, we get the following:

\[\exp \curly*{\eta x^\beta -\brackets*{\brackets*{ \beta - 1 }\log\brackets*{-\eta^{-\frac{1}{\beta}}} -\log \brackets*{ \frac{\beta}{-\eta^{-\frac{1}{\beta}}}  }} + \brackets*{ \beta - 1 } \log\brackets*{x} }\]

where $\eta = -\alpha^{-\beta}$.

\break
*(c) Use the canonical form to:*\\

*i. compute $E(X^{\beta})$.*

\[E(X^{\beta}) = E(T(X))\]

we know that $E(T(X))=\left.A^{\prime}(\eta)\right|_{\eta=\eta(\alpha)}$.\\

First, we will need the first derivative of $\left.A(\eta)\right|_{\eta=\eta(\alpha)}$:

\begin{align*}
&A^{\prime}(\eta) = (\beta-1)  \frac{\brackets*{-\eta^{-\frac{1}{\beta}}}^{\prime}}{-\eta^{-\frac{1}{\beta}}} - \frac{\brackets*{-\eta^{\frac{1}{\beta}}}^{\prime}}{-\eta^{\frac{1}{\beta}}}\\
&= (\beta-1) \brackets*{-\frac{1}{\beta}} \frac{\brackets*{-\eta^{-\frac{1}{\beta}}\eta^{-1}}}{-\eta^{-\frac{1}{\beta}}} -\brackets*{\frac{1}{\beta}} \frac{\brackets*{-\eta^{\frac{1}{\beta}} \eta^{-1} }}{-\eta^{\frac{1}{\beta}}}\\
&= (\beta-1) \brackets*{-\frac{1}{\beta}}\eta^{-1} -\brackets*{\frac{1}{\beta}} \eta^{-1}\\
&= \brackets*{-\frac{1}{\beta} \eta^{-1}} \brackets*{ (\beta-1) + 1}\\
&= \brackets*{-\frac{1}{\beta} \eta^{-1}}\beta \\
&= -\eta^{-1}
\end{align*}

therefore:

\[E(T(X))=\left.A^{\prime}(\eta)\right|_{\eta=-\alpha^{-\beta}} = \alpha^{\beta}\]

*i. and $V(X^{\beta})$.*

\[V(X^{\beta}) = V(T(X))\]

We know that $V(T(X))=\left.A^{\prime\prime}(\eta)\right|_{\eta=\eta(\alpha)} b(\beta)$.\\

First we will need to calculate $A^{\prime\prime}(\eta)$. We already have $A^{\prime}(\eta)$ therefore:


\[A^{\prime\prime}(\eta) = \brackets*{A^{\prime}(\eta)}^{\prime} = \eta^{-2}\]

replacing $\eta$ we get:

\[\left.\eta^{-2}\right|_{\eta = -\alpha^{-\beta}} = \alpha^{2\beta}  \]

Since $b(\beta) = 1$, $V(T(X)) = \alpha^{2\beta}$.

\break
*ii. write the score function $S_n(\alpha)$ and see if it is possible to analytically derive the maximum likelihood estimator of $\alpha$, $\alpha_{MLE}$.*

The score function $S_n(\alpha)$ is given by the following:

\[S_n(\alpha) = \frac{\sum_{i=1}^{n} T(x_i) - nA^{\prime}(\eta)}{b(\beta)} \frac{d\eta}{d\alpha}\]

from the score function we can derive the most likelyhood estimator by solving it equal to zero:

\begin{align*}
&\left.S_n(\eta)\right|_{\eta = -\alpha^{-\beta}} = 0 \Leftrightarrow \sum_{i=1}^{n} \brackets*{T(x_i)} - nA^{\prime}(\alpha) = 0 \\
&\Leftrightarrow \sum_{i=1}^{n} \brackets*{x_i^\beta} - n\alpha^{\beta} = 0 \\
&\Leftrightarrow n\overline{x^{\beta}} - n\alpha^{\beta} = 0\\
&\Leftrightarrow n\overline{x^{\beta}} = n\alpha^{\beta}\\
&\Leftrightarrow \overline{x^{\beta}} = \alpha^{\beta}\\
&\Leftrightarrow \brackets*{\overline{x^{\beta}}}^{\frac{1}{\beta}} = \alpha\\
\end{align*}

Where $\overline{x^{\beta}} = \sum_{i=1}^{n} \brackets*{T(x_i)}$

Therefore the $\alpha_{MLE} = \brackets*{\overline{x^{\beta}}}^{\frac{1}{\beta}}$.\\

\break
*iii. compute the Fisher Information $I_n(\alpha)$.*

The Fisher Information $I_n(\alpha)$ is given by

\[n \left.\frac{A^{\prime \prime}(\eta)}{b(\phi)}\right|_{\eta=\eta(\alpha)} \left(\frac{d \eta}{d \alpha}\right)^2\]

replacing with our values we get:

\begin{align*}
&n \frac{\alpha^{2\beta}}{1} \brackets*{\beta\alpha^{-\beta-1}}^2\\
&= n \brackets*{\alpha^{2\beta}} \brackets*{\beta^2\alpha^{-2\beta-2}}\\
&= n \brackets*{\alpha^{2\beta}} \brackets*{\beta^2\alpha^{-2\beta-2}}\\
&= n \beta^2 \alpha^{-2}
\end{align*}

therefore $I_n(\alpha) = n \beta^2 \alpha^{-2}$

\break
*iv. report the asymptotic variance of the maximum likelihood estimator $\alpha_{MLE}$.*

we know that the maximum likelihood estimator asymptotic variance is given by

\[var(\hat{\alpha}) = I_n(\alpha)^{-1}\]

replacing the values we get

\[var(\alpha_{MLE}) = \frac{1}{n\beta^2\alpha^{-2}}\]


this means that the greater the sample size $n$, the lesser the variance of the $\alpha_{MLE}$
